robots.txt is an extremely simply format, but it still needs some parser loving. This class will normalise robots.txt entries for you.

USAGE
-----

With the following robots.txt:

User-agent: *
Disallow: /logs

User-agent: Google
Disallow: /admin

# Also accepts a local file
rp = RobotsParser.new("http://something.com/robots.txt")

# WILDCARD NOT YET IMPLEMENTED!!!!
rp.user_agents('Google') # returns ["/logs", "/admin"]
rp.user_agents('Google', { :exclude_wildcard => true }) # returns ["/admin"]
